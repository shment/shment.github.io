{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Walmart Dataset Solution","metadata":{}},{"cell_type":"markdown","source":"Walmart [dataset](https://www.kaggle.com/datasets/yasserh/walmart-dataset) consist of the weekly sales in different walmart stores from 2010-02-05 to 2012-11-01. Our goal here is to predict the sales for a given week.\nIn this post we will see how to:\n1. load data stored in a csv file to pandas dataframe\n2. convert categorical to numeric features using scikit learn\n3. handle date feature \n4. learn xgboost model to predict the sales\n5. use optuna for hyper parameters tuning\n6. save and load the model and other auxiliaries for inference on new samples ","metadata":{}},{"cell_type":"markdown","source":"First we will import the libraries we will use:","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport joblib\nimport pandas as pd\nimport xgboost as xgb\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error\nimport optuna","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-01-31T11:03:15.483687Z","iopub.execute_input":"2023-01-31T11:03:15.484118Z","iopub.status.idle":"2023-01-31T11:03:15.491180Z","shell.execute_reply.started":"2023-01-31T11:03:15.484084Z","shell.execute_reply":"2023-01-31T11:03:15.489860Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"markdown","source":"Next we load the data to pandas dataframe and separate the target variable(weekly sales) from the rest.","metadata":{}},{"cell_type":"code","source":"df_x = pd.read_csv('/kaggle/input/walmart-dataset/Walmart.csv')\ndf_y = df_x['Weekly_Sales']\ndf_x = df_x.drop(columns=['Weekly_Sales'])","metadata":{"execution":{"iopub.status.busy":"2023-01-31T11:03:15.493564Z","iopub.execute_input":"2023-01-31T11:03:15.493941Z","iopub.status.idle":"2023-01-31T11:03:15.516792Z","shell.execute_reply.started":"2023-01-31T11:03:15.493907Z","shell.execute_reply":"2023-01-31T11:03:15.515490Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"markdown","source":"Here we check for problems in the data such as nan values and wrong data type.","metadata":{}},{"cell_type":"code","source":"pd.concat([df_x.nunique(axis=0), df_x.isna().sum(axis=0), df_x.dtypes], axis=1).rename(columns={0: 'uniques', 1:'na', 2:'type'})","metadata":{"execution":{"iopub.status.busy":"2023-01-31T11:03:15.519162Z","iopub.execute_input":"2023-01-31T11:03:15.519638Z","iopub.status.idle":"2023-01-31T11:03:15.539645Z","shell.execute_reply.started":"2023-01-31T11:03:15.519594Z","shell.execute_reply":"2023-01-31T11:03:15.538321Z"},"trusted":true},"execution_count":50,"outputs":[{"execution_count":50,"output_type":"execute_result","data":{"text/plain":"              uniques  na     type\nStore              45   0    int64\nDate              143   0   object\nHoliday_Flag        2   0    int64\nTemperature      3528   0  float64\nFuel_Price        892   0  float64\nCPI              2145   0  float64\nUnemployment      349   0  float64","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>uniques</th>\n      <th>na</th>\n      <th>type</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Store</th>\n      <td>45</td>\n      <td>0</td>\n      <td>int64</td>\n    </tr>\n    <tr>\n      <th>Date</th>\n      <td>143</td>\n      <td>0</td>\n      <td>object</td>\n    </tr>\n    <tr>\n      <th>Holiday_Flag</th>\n      <td>2</td>\n      <td>0</td>\n      <td>int64</td>\n    </tr>\n    <tr>\n      <th>Temperature</th>\n      <td>3528</td>\n      <td>0</td>\n      <td>float64</td>\n    </tr>\n    <tr>\n      <th>Fuel_Price</th>\n      <td>892</td>\n      <td>0</td>\n      <td>float64</td>\n    </tr>\n    <tr>\n      <th>CPI</th>\n      <td>2145</td>\n      <td>0</td>\n      <td>float64</td>\n    </tr>\n    <tr>\n      <th>Unemployment</th>\n      <td>349</td>\n      <td>0</td>\n      <td>float64</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"The data don't have nan values, but the *Store* and *Holiday_Flag* features need to be categorical - there is no sense in \"store 1 is bigger then store 0\". In addition, the *Date* feature has order - 2010-02-05 is before 2010-02-06, so we need to convert it to numerical feature. Simple way for doing that is to create 3 new features - *day*, *month* and *year*. Here we predict the weekly sales, so no additional processing is needed, however if we needed to predict daily sales, since there is likely a correlation between the week day and the sales, we could add another feature for the weekday (Sunday, Monday,...). ","metadata":{}},{"cell_type":"code","source":"df_x['Store'] = df_x['Store'].astype(object)\ndf_x['Holiday_Flag'] = df_x['Holiday_Flag'].astype(object)\n\ndf_x['Date'] = pd.to_datetime(df_x['Date'])\ndf_x['day'] = df_x['Date'].dt.day\ndf_x['month'] = df_x['Date'].dt.month\ndf_x['year'] = df_x['Date'].dt.year\n\ndf_x = df_x.drop(columns=['Date'])","metadata":{"execution":{"iopub.status.busy":"2023-01-31T11:03:15.541656Z","iopub.execute_input":"2023-01-31T11:03:15.542098Z","iopub.status.idle":"2023-01-31T11:03:15.559098Z","shell.execute_reply.started":"2023-01-31T11:03:15.542042Z","shell.execute_reply":"2023-01-31T11:03:15.558184Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"markdown","source":"Split the data to train and test sets.","metadata":{}},{"cell_type":"code","source":"x_train, x_test, y_train, y_test = train_test_split(df_x, df_y, test_size=0.1, random_state=1)","metadata":{"execution":{"iopub.status.busy":"2023-01-31T11:03:15.561821Z","iopub.execute_input":"2023-01-31T11:03:15.563018Z","iopub.status.idle":"2023-01-31T11:03:15.574070Z","shell.execute_reply.started":"2023-01-31T11:03:15.562970Z","shell.execute_reply":"2023-01-31T11:03:15.572869Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"markdown","source":"In the first line of the next cell, we get the list of the categorical features names. \nIn order to convert the categorical features to numeric ones, we use one hot encoding.","metadata":{}},{"cell_type":"code","source":"categorical_features = list(x_train.loc[:, x_train.dtypes == object])\ncat_encoder = OneHotEncoder()\ncat_encoder.fit(x_train.loc[:, categorical_features])\ntransformed = cat_encoder.transform(x_train.loc[:, categorical_features].to_numpy())\nohe = pd.DataFrame(transformed.toarray(), columns=cat_encoder.get_feature_names_out())\nx_train = x_train.reset_index()\nx_train = pd.concat([x_train, ohe], axis=1)\nx_train = x_train.drop(columns=categorical_features)","metadata":{"execution":{"iopub.status.busy":"2023-01-31T11:03:15.575434Z","iopub.execute_input":"2023-01-31T11:03:15.576551Z","iopub.status.idle":"2023-01-31T11:03:15.600105Z","shell.execute_reply.started":"2023-01-31T11:03:15.576516Z","shell.execute_reply":"2023-01-31T11:03:15.599240Z"},"trusted":true},"execution_count":53,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/base.py:451: UserWarning: X does not have valid feature names, but OneHotEncoder was fitted with feature names\n  \"X does not have valid feature names, but\"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Now our data is ready to use. \nNext thing we do is to search for the best hyper parameters for our xgboost model. The way we do that with optuna is by implementing *objective* function that will output a number corresponding with how good the chosen hyperparameters were. Here we output the mean absolute error.\nThen we need to create a *study* object. Since lower mean absolute error is better we give it *direction=\"minimize\"*. The *n_trials* parameter needs to be as high as you can afford. In order to make this cell run faster, I set it to 1. ","metadata":{}},{"cell_type":"code","source":"def objective(trial, x, y):\n    xx_train, xx_val, yy_train, yy_val = train_test_split(x, y, test_size=0.1, random_state=1)\n    dtrain = xgb.DMatrix(xx_train, label=yy_train)\n    dval = xgb.DMatrix(xx_val, label=yy_val)\n\n    param = {\n        \"objective\": \"reg:squarederror\",\n        \"booster\": trial.suggest_categorical(\"booster\", [\"gbtree\", \"dart\"]),\n        \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 1.0, log=True),\n        \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 1.0, log=True),\n        \"max_depth\": trial.suggest_int(\"max_depth\", 5, 20),\n        \"gamma\": trial.suggest_float(\"gamma\", 1e-8, 1.0, log=True),\n        \"grow_policy\": trial.suggest_categorical(\"grow_policy\", [\"depthwise\", \"lossguide\"]),\n        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-5, 1.0, log=True)\n    }\n\n    bst = xgb.train(param, dtrain, num_boost_round=100)\n    preds = bst.predict(dval)\n    loss = mean_absolute_error(yy_val, preds)\n    return loss\n\nstudy = optuna.create_study(direction=\"minimize\")\nstudy.optimize(lambda trial: objective(trial, x_train, y_train), n_trials=1)\nprint(\"Number of finished trials: \", len(study.trials))\nprint(\"Best trial:\")\ntrial = study.best_trial\n\nprint(\"  Value: {}\".format(trial.value))\nprint(\"  Params: \")\nfor key, value in trial.params.items():\n    print(\"    {}: {}\".format(key, value))","metadata":{"execution":{"iopub.status.busy":"2023-01-31T11:03:15.601349Z","iopub.execute_input":"2023-01-31T11:03:15.601871Z","iopub.status.idle":"2023-01-31T11:03:21.377048Z","shell.execute_reply.started":"2023-01-31T11:03:15.601841Z","shell.execute_reply":"2023-01-31T11:03:21.376089Z"},"trusted":true},"execution_count":54,"outputs":[{"name":"stderr","text":"\u001b[32m[I 2023-01-31 11:03:15,609]\u001b[0m A new study created in memory with name: no-name-4ae42f95-a3de-4778-a88a-8390f0ec854a\u001b[0m\n\u001b[32m[I 2023-01-31 11:03:21,369]\u001b[0m Trial 0 finished with value: 1031799.4268691194 and parameters: {'booster': 'dart', 'lambda': 0.3953833400194366, 'alpha': 0.22968793233850288, 'max_depth': 16, 'gamma': 1.6650027414161305e-05, 'grow_policy': 'lossguide', 'learning_rate': 2.5312771673393158e-05}. Best is trial 0 with value: 1031799.4268691194.\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"Number of finished trials:  1\nBest trial:\n  Value: 1031799.4268691194\n  Params: \n    booster: dart\n    lambda: 0.3953833400194366\n    alpha: 0.22968793233850288\n    max_depth: 16\n    gamma: 1.6650027414161305e-05\n    grow_policy: lossguide\n    learning_rate: 2.5312771673393158e-05\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Now that we have the hyper parameters for our model, we can learn one.\nThe hyper parameters specified here are the output optuna gave after 100 trails. Usually higher *n_estimators* results in a better model, so we increase it to the largest value we can afford.","metadata":{}},{"cell_type":"code","source":"model = xgb.XGBRegressor(objective='reg:squarederror',\n                         n_estimators=1000,\n                         reg_lambda=0.2570131417683139,\n                         alpha=0.0036109145652048207,\n                         max_depth=8,\n                         gamma=0.0054382741007995014,\n                         grow_policy='lossguide',\n                         booster='dart',\n                         learning_rate=0.1618795141355424)\nmodel.fit(x_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2023-01-31T11:03:21.381282Z","iopub.execute_input":"2023-01-31T11:03:21.382613Z","iopub.status.idle":"2023-01-31T11:10:37.842594Z","shell.execute_reply.started":"2023-01-31T11:03:21.382562Z","shell.execute_reply":"2023-01-31T11:10:37.841280Z"},"trusted":true},"execution_count":55,"outputs":[{"execution_count":55,"output_type":"execute_result","data":{"text/plain":"XGBRegressor(alpha=0.0036109145652048207, base_score=0.5, booster='dart',\n             callbacks=None, colsample_bylevel=1, colsample_bynode=1,\n             colsample_bytree=1, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric=None,\n             gamma=0.0054382741007995014, gpu_id=-1, grow_policy='lossguide',\n             importance_type=None, interaction_constraints='',\n             learning_rate=0.1618795141355424, max_bin=256, max_cat_to_onehot=4,\n             max_delta_step=0, max_depth=8, max_leaves=0, min_child_weight=1,\n             missing=nan, monotone_constraints='()', n_estimators=1000,\n             n_jobs=0, num_parallel_tree=1, predictor='auto', random_state=0,\n             reg_alpha=0.00361091457, ...)"},"metadata":{}}]},{"cell_type":"markdown","source":"Now that the model is ready, we can save it and the one hot encoder for future use.","metadata":{}},{"cell_type":"code","source":"joblib.dump(model, 'model.joblib')\njoblib.dump(cat_encoder, 'cat_encoder.joblib')\njoblib.dump(categorical_features, 'categorical_features.joblib')","metadata":{"execution":{"iopub.status.busy":"2023-01-31T11:10:37.844142Z","iopub.execute_input":"2023-01-31T11:10:37.844488Z","iopub.status.idle":"2023-01-31T11:10:37.895366Z","shell.execute_reply.started":"2023-01-31T11:10:37.844458Z","shell.execute_reply":"2023-01-31T11:10:37.894463Z"},"trusted":true},"execution_count":56,"outputs":[{"execution_count":56,"output_type":"execute_result","data":{"text/plain":"['categorical_features.joblib']"},"metadata":{}}]},{"cell_type":"markdown","source":"Here is how to use the model on new data.\nWe load the model and encoder, transform the data the same way we did for train set.\n","metadata":{}},{"cell_type":"code","source":"model = joblib.load('model.joblib')\ncat_encoder = joblib.load('cat_encoder.joblib')\ncategorical_features = joblib.load('categorical_features.joblib')\n\ndf_inference = x_test.copy()\n\ntransformed = cat_encoder.transform(x_test.loc[:, categorical_features].to_numpy())\nohe = pd.DataFrame(transformed.toarray(), columns=cat_encoder.get_feature_names_out())\nx_test = x_test.reset_index()\nx_test = pd.concat([x_test, ohe], axis=1)\nx_test = x_test.drop(columns=categorical_features)","metadata":{"execution":{"iopub.status.busy":"2023-01-31T11:10:37.897809Z","iopub.execute_input":"2023-01-31T11:10:37.898452Z","iopub.status.idle":"2023-01-31T11:10:37.949199Z","shell.execute_reply.started":"2023-01-31T11:10:37.898418Z","shell.execute_reply":"2023-01-31T11:10:37.947792Z"},"trusted":true},"execution_count":57,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/sklearn/base.py:451: UserWarning: X does not have valid feature names, but OneHotEncoder was fitted with feature names\n  \"X does not have valid feature names, but\"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Now we can use our model to predict weekly sales of the new data.\nFor debuging, we can save our predictions in a csv file.","metadata":{}},{"cell_type":"code","source":"test_pred = model.predict(x_test)\ndf_inference['pred'] = test_pred\ndf_inference.to_csv('df_inference.csv', index=False)\nprint('mean_absolute_error:', mean_absolute_error(test_pred, y_test))\nprint('mean_absolute_percentage_error:', mean_absolute_percentage_error(test_pred, y_test))","metadata":{"execution":{"iopub.status.busy":"2023-01-31T11:10:37.950599Z","iopub.execute_input":"2023-01-31T11:10:37.950941Z","iopub.status.idle":"2023-01-31T11:10:38.216792Z","shell.execute_reply.started":"2023-01-31T11:10:37.950911Z","shell.execute_reply":"2023-01-31T11:10:38.215950Z"},"trusted":true},"execution_count":58,"outputs":[{"name":"stdout","text":"40504.16714091614\n0.03825470121410209\n","output_type":"stream"}]}]}